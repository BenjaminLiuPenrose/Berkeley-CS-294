# Meta Reinforcement Learning
## Problem 1 - Context as Task ID
#### learn a simple contextual policy
-  Include a plot of the average return
![hist](data/Q1.png)

## Problem 2 - Meta-Learned Context
#### Compare the performance of the feed-forward and recurrent architectures for different lengths of history
- Include a plot of average return for both architectures for at least three different history lengths.

-Discuss your results. What minimum history length is needed? Which architecture works better? If you change any hyper-parameters, discuss the result.
![rbf](data/Q2.png)

## Problem 3 - Generalization
#### Compare the performance of the policy on training goals and testing goals.
- Vary the granularity of the checkerboard and comment on the generalization performance as the training and testing distributions become more different.

- Include plots comparing training and testing average returns for at least two different settings.

