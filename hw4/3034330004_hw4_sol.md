# Problem 1
## a) Provide a plot of the dynamics model predictions when the predictions are mostly accurate
![model prediction](data/q1.jpg)
## b) For (a), for which state dimension are the predictions the most inaccurate? Give a possible reason why the predictions are inaccurate
- Prediction for state dim-17 is the most inaccurate. Since it is a open loop prediction, the small error will compound and the cumulation of them will deviate a lot from the actual states.

# Problem 2
## Provide the ReturnAvg and ReturnStd for the random policy and for your model-based controller trained on the randomly gathered data.
|   |                     |                    |                     |                     |                    |                     |
|---|---------------------|--------------------|---------------------|---------------------|--------------------|---------------------|
|   | ReturnAvg           | ReturnStd          | ReturnMin           | ReturnMax           | TrainingLossStart  | TrainingLossFinal   |
| 0 | -144.93757095287015 | 24.684182445040772 | -183.29364256233976 | -109.52336525035639 |                    |                     |
| 1 | 31.749897766043723  | 22.972627725335872 | -6.697796649236718  | 64.31774153765512   | 1.0188288688659668 | 0.02625294402241707 |


# Problem 3a
## Plot of the returns versus iteration when running model-based reinforcement learning.
|    |     |                     |                    |                     |                     |                      |                      |
|----|-----|---------------------|--------------------|---------------------|---------------------|----------------------|----------------------|
|    | Itr | ReturnAvg           | ReturnStd          | ReturnMin           | ReturnMax           | TrainingLossStart    | TrainingLossFinal    |
| 0  | -1  | -162.21072832050314 | 20.677899585472954 | -194.29865752474814 | -133.4355296160121  |                      |                      |
| 1  | 0   | -149.1360855936979  | 45.805762222240794 | -194.29865752474814 | -18.389658325645755 | 1.0143616199493408   | 0.023922434076666832 |
| 2  | 1   | -125.56065076362819 | 89.65005818956617  | -194.29865752474814 | 133.76913236713878  | 0.02717895805835724  | 0.01721280626952648  |
| 3  | 2   | -109.49252297911076 | 102.55295561080246 | -194.29865752474814 | 133.76913236713878  | 0.01868356391787529  | 0.01397019438445568  |
| 4  | 3   | -94.76528808312044  | 112.18500556692398 | -194.29865752474814 | 133.76913236713878  | 0.016327109187841415 | 0.01166887953877449  |
| 5  | 4   | -84.35959762788481  | 115.16223899214212 | -194.29865752474814 | 133.76913236713878  | 0.014894505962729454 | 0.012229773215949535 |
| 6  | 5   | -70.6065022459166   | 123.57450970107382 | -194.29865752474814 | 135.68992848360662  | 0.011270676739513874 | 0.011246797628700733 |
| 7  | 6   | -57.73649565013987  | 130.47060388876065 | -194.29865752474814 | 148.18360988228773  | 0.011038070544600487 | 0.009313054382801056 |
| 8  | 7   | -46.998182941065565 | 134.30254201595477 | -194.29865752474814 | 148.18360988228773  | 0.01082630455493927  | 0.008895021863281727 |
| 9  | 8   | -38.698259234511895 | 135.38037195909754 | -194.29865752474814 | 148.18360988228773  | 0.01046840101480484  | 0.011161689646542072 |
| 10 | 9   | -27.866544321097514 | 140.14509541021621 | -194.29865752474814 | 177.93603903377567  | 0.011216932907700539 | 0.008859279565513134 |
| 11 | 10  | -18.319359926328406 | 143.2771908699417  | -194.29865752474814 | 177.93603903377567  | 0.009269101545214653 | 0.008101445622742176 |



# Problem 3b
## a) Plot comparing performance when varying the MPC horizon.

## b) Plot comparing performance when varying the number of randomly sampled action sequences used for planning.

## c) Plot comparing performance when varying the number of neural network layers for the learned dynamics model.






