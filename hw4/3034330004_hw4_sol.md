# Problem 1
### a) Provide a plot of the dynamics model predictions when the predictions are mostly accurate
![model prediction](plots/state_pred.jpg)

### b) For (a), for which state dimension are the predictions the most inaccurate? Give a possible reason why the predictions are inaccurate
- Prediction for state dim-17 is the most inaccurate. Since it is a open loop prediction and state 17 has a upward trend, the small prediction errors will compound and the accumulation of them will make prediction deviate a lot from the actual states.

# Problem 2
### Provide the ReturnAvg and ReturnStd for the random policy and for your model-based controller trained on the randomly gathered data.
- 0 is random policy
- 1 is model based controller
- 
![table](plots/table.png)

|   | ReturnAvg           | ReturnStd          | ReturnMin           | ReturnMax           | TrainingLossStart  | TrainingLossFinal   |
| 0 | -144.93757095287015 | 24.684182445040772 | -183.29364256233976 | -109.52336525035639 |                    |                     |
| 1 | 31.749897766043723  | 22.972627725335872 | -6.697796649236718  | 64.31774153765512   | 1.0188288688659668 | 0.02625294402241707 |


# Problem 3a
### Plot of the returns versus iteration when running model-based reinforcement learning.
![default](plots/HalfCheetah_q3_default.jpg)




# Problem 3b
### a) Plot comparing performance when varying the MPC horizon.
![mpc horizons](plots/HalfCheetah_q3_mpc_horizon.jpg)

### b) Plot comparing performance when varying the number of randomly sampled action sequences used for planning.
![actions](plots/HalfCheetah_q3_actions.jpg)

### c) Plot comparing performance when varying the number of neural network layers for the learned dynamics model.
![nn layers](plots/HalfCheetah_q3_nn_layers.jpg)

# Extra Bonus
### Plot comparing performance of either CEM to random for action selection. 
- code is implemented in `model_based_policy.py line 204-line 233`

![table](plots/table1.png)





