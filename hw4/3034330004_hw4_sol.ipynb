{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "### a) Provide a plot of the dynamics model predictions when the predictions are mostly accurate\n",
    "![model prediction](plots/state_pred.jpg)\n",
    "\n",
    "### b) For (a), for which state dimension are the predictions the most inaccurate? Give a possible reason why the predictions are inaccurate\n",
    "- Prediction for state dim-17 is the most inaccurate. Since it is a open loop prediction and state 17 has a upward trend, the small prediction errors will compound and the accumulation of them will make prediction deviate a lot from the actual states.\n",
    "\n",
    "# Problem 2\n",
    "### Provide the ReturnAvg and ReturnStd for the random policy and for your model-based controller trained on the randomly gathered data.\n",
    "- 0 is random policy\n",
    "- 1 is model based controller\n",
    "- \n",
    "![table](plots/table.png)\n",
    "\n",
    "|   | ReturnAvg           | ReturnStd          | ReturnMin           | ReturnMax           | TrainingLossStart  | TrainingLossFinal   |\n",
    "| 0 | -144.93757095287015 | 24.684182445040772 | -183.29364256233976 | -109.52336525035639 |                    |                     |\n",
    "| 1 | 31.749897766043723  | 22.972627725335872 | -6.697796649236718  | 64.31774153765512   | 1.0188288688659668 | 0.02625294402241707 |\n",
    "\n",
    "\n",
    "# Problem 3a\n",
    "### Plot of the returns versus iteration when running model-based reinforcement learning.\n",
    "![default](plots/HalfCheetah_q3_default.jpg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Problem 3b\n",
    "### a) Plot comparing performance when varying the MPC horizon.\n",
    "![mpc horizons](plots/HalfCheetah_q3_mpc_horizon.jpg)\n",
    "\n",
    "### b) Plot comparing performance when varying the number of randomly sampled action sequences used for planning.\n",
    "![actions](plots/HalfCheetah_q3_actions.jpg)\n",
    "\n",
    "### c) Plot comparing performance when varying the number of neural network layers for the learned dynamics model.\n",
    "![nn layers](plots/HalfCheetah_q3_nn_layers.jpg)\n",
    "\n",
    "# Extra Bonus\n",
    "### Plot comparing performance of either CEM to random for action selection. \n",
    "- code is implemented in `model_based_policy.py line 204-line 233`\n",
    "\n",
    "![table](plots/table1.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
