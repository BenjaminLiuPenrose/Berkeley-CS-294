--env_type gym
--env_name ___
remark {}NoFrameskip-v4 --no frame skipping on environment site
env_name
from gym import envs
all_envs = envs.registry.all()
env_ids = [env_spec.id for env_spec in all_envs if "NoFrameskip-v4" in env_spec.id and "-ram" not in env_spec.id]
['AirRaidNoFrameskip-v4', 'AlienNoFrameskip-v4', 'AmidarNoFrameskip-v4', 'AssaultNoFrameskip-v4', 'AsterixNoFrameskip-v4', 'AsteroidsNoFrameskip-v4', 'AtlantisNoFrameskip-v4', 'BankHeistNoFrameskip-v4', 'BattleZoneNoFrameskip-v4', 'BeamRiderNoFrameskip-v4', 'BerzerkNoFrameskip-v4', 'BowlingNoFrameskip-v4', 'BoxingNoFrameskip-v4', 'BreakoutNoFrameskip-v4', 'CarnivalNoFrameskip-v4', 'CentipedeNoFrameskip-v4', 'ChopperCommandNoFrameskip-v4', 'CrazyClimberNoFrameskip-v4', 'DemonAttackNoFrameskip-v4', 'DoubleDunkNoFrameskip-v4', 'ElevatorActionNoFrameskip-v4', 'EnduroNoFrameskip-v4', 'FishingDerbyNoFrameskip-v4', 'FreewayNoFrameskip-v4', 'FrostbiteNoFrameskip-v4', 'GopherNoFrameskip-v4', 'GravitarNoFrameskip-v4', 'HeroNoFrameskip-v4', 'IceHockeyNoFrameskip-v4', 'JamesbondNoFrameskip-v4', 'JourneyEscapeNoFrameskip-v4', 'KangarooNoFrameskip-v4', 'KrullNoFrameskip-v4', 'KungFuMasterNoFrameskip-v4', 'MontezumaRevengeNoFrameskip-v4', 'MsPacmanNoFrameskip-v4', 'NameThisGameNoFrameskip-v4', 'PhoenixNoFrameskip-v4', 'PitfallNoFrameskip-v4', 'PongNoFrameskip-v4', 'PooyanNoFrameskip-v4', 'PrivateEyeNoFrameskip-v4', 'QbertNoFrameskip-v4', 'RiverraidNoFrameskip-v4', 'RoadRunnerNoFrameskip-v4', 'RobotankNoFrameskip-v4', 'SeaquestNoFrameskip-v4', 'SkiingNoFrameskip-v4', 'SolarisNoFrameskip-v4', 'SpaceInvadersNoFrameskip-v4', 'StarGunnerNoFrameskip-v4', 'TennisNoFrameskip-v4', 'TimePilotNoFrameskip-v4', 'TutankhamNoFrameskip-v4', 'UpNDownNoFrameskip-v4', 'VentureNoFrameskip-v4', 'VideoPinballNoFrameskip-v4', 'WizardOfWorNoFrameskip-v4', 'YarsRevengeNoFrameskip-v4', 'ZaxxonNoFrameskip-v4']
Pong-v0
Pong-v4 The difference between v0 and v4 is repeat_action_probabilitythe difference. v4 p=0.0
PongDeterministic-v4 NoFrameskip: No action repeated
PongNoFrameskip-v4 NoFrameskip: No action repeated
Pong-ram-v4 ram: observationReply the state of internal RAM as observation value
Pong-ramDeterministic-v0
Pong-ramDeterministic-v4
Pong-ramNoFrameskip-v0
Pong-ramNoFrameskip-v4


NoFrameskip-v4
Game	DQN	Gorila	Double	Dueling	Prioritized	A3C	FF*	A3C	FF	A3C	LSTM
Alien	570.2	813.5	1033.4	1486.5	900.5	182.1	518.4	945.3  480*
Amidar	133.4	189.2	169.1	172.7	218.4	283.9	263.9	173.0  160
Assault	3332.3	1195.8	6060.8	3994.8	7748.5	3746.1	5474.9	14497.9
Asterix	124.5	3324.7	16837.0	15840.0	31907.5	6723.0	22140.5	17244.5
Asteroids	697.1	933.6	1193.2	2035.4	1654.0	3009.4	4474.5	5093.1
Atlantis	76108.0	629166.5	319688.0	445360.0	593642.0	772392.0	911091.0	875822.0
Bank	Heist	176.3	399.4	886.0	1129.3	816.8	946.0	970.1	932.8
Battle	Zone	17560.0	19938.0	24740.0	31320.0	29100.0	11340.0	12950.0	20760.0
Beam	Rider	8672.4	3822.1	17417.2	14591.3	26172.7	13235.9	22707.9	24622.2
Berzerk	1011.1	910.6	1165.6	1433.4	817.9	862.2
Bowling	41.2	54.0	69.6	65.7	65.8	36.2	35.1	41.8
Boxing	25.8	74.2	73.5	77.3	68.6	33.7	59.8	37.3
Breakout	303.9	313.0	368.9	411.6	371.6	551.6	681.9	766.8
Centipede	3773.1	6296.9	3853.5	4881.0	3421.9	3306.5	3755.8	1997.0
ChopperComman	3046.0	3191.8	3495.0	3784.0	6604.0	4669.0	7021.0	10150.0
CrazyClimber	50992.0	65451.0	113782.0	124566.0	131086.0	101624.0	112646.0	138518.0
Defender	27510.0	33996.0	21093.5	36242.5	56533.0	233021.5
DemonAttack	12835.2	14880.1	69803.4	56322.8	73185.8	84997.5	113308.4	115201.9
DoubleDunk	-21.6	-11.3	-0.3	-0.8	2.7	0.1	-0.1	0.1
Enduro	475.6	71.0	1216.6	2077.4	1884.4	-82.2	-82.5	-82.5
FishingDerby	-2.3	4.6	3.2	-4.1	9.2	13.6	18.8	22.6
Freeway	25.8	10.2	28.8	0.2	27.9	0.1	0.1	0.1
Frostbite	157.4	426.6	1448.1	2332.4	2930.2	180.1	190.5	197.6
Gopher	2731.8	4373.0	15253.0	20051.4	57783.8	8442.8	10022.8	17106.8
Gravitar	216.5	538.4	200.5	297.0	218.0	269.5	303.5	320.0
H.E.R.O.	12952.5	8963.4	14892.5	15207.9	20506.4	28765.8	32464.1	28889.5
IceHockey	-3.8	-1.7	-2.5	-1.3	-1.0	-4.7	-2.8	-1.7
JamesBond	348.5	444.0	573.0	835.5	3511.5	351.5	541.0	613.0
Kangaroo	2696.0	1431.0	11204.0	10334.0	10241.0	106.0	94.0	125.0
Krull	3864.0	6363.1	6796.1	8051.6	7406.5	8066.6	5560.0	5911.4
Kung-FuMaster	11875.0	20620.0	30207.0	24288.0	31244.0	3046.0	28819.0	40835.0
Montezuma’s	Revenge	50.0	84.0	42.0	22.0	13.0	53.0	67.0	41.0
Ms.Pacman	763.5	1263.0	1241.3	2250.6	1824.6	594.4	653.7	850.7
NameThisGame	5439.9	9238.5	8960.3	11185.1	11836.1	5614.0	10476.1	12093.7
Phoenix	12366.5	20410.5	27430.1	28181.8	52894.1	74786.7
PitFall	-186.7	-46.9	-14.8	-123.0	-78.5	-135.7
Pong	16.2	16.7	19.1	18.8	18.9	11.4	5.6	10.7 21
PrivateEye	298.2	2598.6	-575.5	292.6	179.0	194.4	206.9	421.1
Q*Bert	4589.8	7089.8	11020.8	14175.8	11277.0	13752.3	15148.8	21307.5
RiverRaid	4065.3	5310.3	10838.4	16569.4	18184.4	10001.2	12201.8	6591.9
RoadRunner	9264.0	43079.8	43156.0	58549.0	56990.0	31769.0	34216.0	73949.0
Robotank	58.5	61.8	59.1	62.0	55.4	2.3	32.8	2.6
Seaquest	2793.9	10145.9	14498.0	37361.6	39096.7	2300.2	2355.4	1326.1
Skiing	-11490.4	-11928.0	-10852.8	-13700.0	-10911.1	-14863.8
Solaris	810.0	1768.4	2238.2	1884.8	1956.0	1936.4
SpaceInvaders	1449.7	1183.3	2628.7	5993.1	9063.0	2214.7	15730.5	23846.0
StarGunner	34081.0	14919.2	58365.0	90804.0	51959.0	64393.0	138218.0	164766.0
Surround	1.9	4.0	-0.9	-9.6	-9.7	-8.3
Tennis	-2.3	-0.7	-7.8	4.4	-2.0	-10.2	-6.3	-6.4
Time	Pilot	5640.0	8267.8	6608.0	6601.0	7448.0	5825.0	12679.0	27202.0
Tutankham	32.4	118.5	92.2	48.0	33.6	26.1	156.3	144.2
UpandDown	3311.3	8747.7	19086.9	24759.2	29443.7	54525.4	74705.7	105728.7
Venture	54.0	523.4	21.0	200.0	244.0	19.0	23.0	25.0
VideoPinball	20228.1	112093.4	367823.7	110976.2	374886.9	185852.6	331628.1	470310.5
WizardofWor	246.0	10431.0	6201.0	7054.0	7451.0	5278.0	17244.0	18082.0
YarsRevenge	6270.6	25976.5	5965.1	7270.8	7157.5	5615.5
Zaxxon	831.0	6159.4	8593.0	10164.0	9501.0	2659.0	24622.0	23519.0

--env_type maze
--env_name maze

--env_type lab
--env_name Labyrinth, seekavoid_arena_01, nav_maze_static_01
We performed an additional set of experiments with A3C on a new 3D environment called
Labyrinth. The specific task we considered involved the agent learning to find rewards in
randomly generated mazes. At the beginning of each episode the agent was placed in a
new randomly generated maze consisting of rooms and corridors. Each maze contained two
types of objects that the agent was rewarded for finding – apples and portals. Picking up
the agent lead to a reward of 1. Entering a portal lead to a reward of 10 after which the
agent was respawned in a new random location in the maze and all previously collected
apples were regenerated. An episode terminated after 60 seconds after which a new episode
would begin. The aim of the agent is to collect as many points as possible in the time
limit and the optimal strategy involves first finding the portal and then repeatedly going
back to it after each respawn. This task is much more challenging than the TORCS driving
domain because the agent is faced with a new maze in each episode and must learn a general
strategy for exploring random mazes.
Game	A3C
Labyrinth	50(200m)

# logging rewards , see in tensorboard
# plot videoframe , see in frame_save_dir
# visualize network , see in popup matplotlib

add new tasks
- UnrealModel._create_network._create_rp_network
- Trainer.process._process_rp
- main

- setup.py
- cloudml-gpu.yaml


