\documentclass[12pt]{article}
\usepackage{fullpage}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{color}
\usepackage{listings}
\usepackage{verbatim}
\usepackage{enumitem}
\usepackage[parfill]{parskip}

\newcommand{\xb}{\mathbf{x}}
\newcommand{\yb}{\mathbf{y}}
\newcommand{\wb}{\mathbf{w}}
\newcommand{\Xb}{\mathbf{X}}
\newcommand{\Yb}{\mathbf{Y}}
\newcommand{\tr}{^T}
\newcommand{\hb}{\mathbf{h}}
\newcommand{\Hb}{\mathbf{H}}

\newcommand{\cmt}[1]{{\footnotesize\textcolor{red}{#1}}}
\newcommand{\todo}[1]{\cmt{TO-DO: #1}}

\title{CS294-112 Deep Reinforcement Learning HW2: \\ Policy Gradients\\
\textbf{due September 19th 2018, 11:59 pm}}

\author{
}

\date{}

\usepackage{courier}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize\ttfamily,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    %numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}

\begin{document}


\maketitle


\textbf{Problem 1. State-dependent baseline:}
In lecture we saw that the policy gradient is unbiased if the baseline is a constant with respect to $\tau$ (Equation~\ref{constant_wrt_tau}). The purpose of this problem is to help convince ourselves that subtracting a state-dependent baseline from the return keeps the policy gradient unbiased. Using the \href{https://en.wikipedia.org/wiki/Law_of_total_expectation}{\textcolor{blue}{law of iterated expectations}} show that the policy gradient is still unbiased if the baseline $b$ is function of a state at a particular timestep of $\tau$ (Equation~\ref{state_dependent_baseline}).
Please answer the questions below in \LaTeX in your report.
\begin{enumerate} [label=(\alph*)]
\item Note that by \href{https://brilliant.org/wiki/linearity-of-expectation/}{\textcolor{blue}{linearity of expectation}} the objective can be written as:
\begin{align*}
    J(\theta) &= \mathbb{E}_{\tau \sim \pi_\theta(\tau)} \left[r(\tau)\right] \\
    &= \mathbb{E}_{\tau \sim \pi_\theta(\tau)} \left[\sum_{t=1}^{T} r(s_t, a_t)\right] \\
    &= \sum_{t=1}^{T} \mathbb{E}_{(s_t, a_t) \sim p(s_t, a_t)} \left[r(s_t, a_t)\right] \\
\end{align*}
when we subtract the baseline $b(s_t)$, the objective becomes:
\begin{align*}
    &= \sum_{t=1}^{T} \mathbb{E}_{(s_t, a_t) \sim p(s_t, a_t)} \left[r(s_t, a_t) - b(s_t)\right].
\end{align*}
Please show that
\begin{align*}
\nabla_\theta \sum_{t=1}^{T} \mathbb{E}_{(s_t, a_t) \sim p(s_t, a_t)} \left[b(s_t)\right] = 0.
\end{align*}

\item Solution to (a): \\
Assume $a_t$ and $s_t$ are discrete variables.
The trajectory follows  $p(s_t, a_t)$.
The policy is $pi_\theta(a_t|s_t)$.
The state transition probability is $p(s_t|s_{t-1},a_{t-1})$.
Notice that given at time t, $s_{t-1}$ and $a_{t-1}$ are known
\begin{align*}
    J(\theta) &= \sum_{t=1}^{T} \mathbb{E}_{(s_t, a_t) \sim p_\theta(s_t, a_t)} \left[b(s_t)\right] \\
    &= \sum_{t=1}^{T} \sum_{s_t}{ \sum_{a_t}{b(s_t)\pi_\theta(a_t|s_t)p(s_t|s_{t-1},a_{t-1})} } \\
    &= \sum_{t=1}^{T} \sum_{s_t}{ b(s_t)p(s_t|s_{t-1},a_{t-1})  \sum_{a_t} {\pi_\theta(a_t|s_t)}} \\
    &= \sum_{t=1}^{T} \sum_{s_t}{ b(s_t)p(s_t|s_{t-1},a_{t-1})}\\
\end{align*}
Above is independent of $\theta$, then 
\begin{align*}
    \nabla_\theta J(\theta) &= \nabla_\theta \sum_{t=1}^{T} \sum_{s_t}{ b(s_t)p(s_t|s_{t-1},a_{t-1})} =0 	
\end{align*}



\item An alternative approach is to look at the entire trajectory and consider a particular timestep $t^* \in [1, T-1]$ (the timestep $T$ case would be very similar to part (a)).
\begin{enumerate}
    \item We can exploit the conditional independency structure of $\pi_\theta(\tau) = p(s_1, a_1, ..., s_T, a_T)$ and use the law of iterated expectations to break Equation~\ref{unbiased_state_dependent_baseline} into two expectations, where the the outer expectation is over $(s_1, a_1, ..., a_{t^*-1}, s_{t^*})$, and the inner expectation is over the rest of the trajectory, conditioned on $(s_1, a_1, ..., a_{t^*-1}, s_{t^*})$. Explain why, for the inner expectation, conditioning on $(s_1, a_1, ..., a_{t^*-1}, s_{t^*})$ is equivalent to conditioning only on $s_{t^*}$.
    \item Using the iterated expectation described above, show that
\begin{align} \label{unbiased_state_dependent_baseline}
\nabla_\theta \mathbb{E}_{\tau \sim \pi_\theta(\tau)} \left[b\left(s_{t^*}\right)\right] = 0.
\end{align}
\end{enumerate}

\item Solution to (c):\\
Denote $\tau^*$ as $(s_1, a_1, ..., a_{t^*-1}, s_{t^*})$ \\
Denote $\tau^c$ as $(a_{t^*}, s_{t^*+1}, ..., a_{T}, s_{T})$ \\
\begin{align*}
    &\mathbb{E}_{\tau \sim p_\theta(\tau)} \left[b\left(s_{t^*}\right)\right] = \mathbb{E}_{\tau} \left[b\left(s_{t^*}\right)\right] \\
    &= \sum_{\tau^C} {\sum_{\tau^*} {b(s_{t^*})p(\tau^*)p(\tau|\tau^*)}}
\end{align*}
Notice that $p(\tau|\tau^*)=p(\tau|s_{t^*})=p(\tau^C)$ due to Markov Property of the MDP.
\begin{align*}
	&= \sum_{\tau^C} {\sum_{\tau^*} {b(s_{t^*})p(\tau^*)p(\tau^C )}}
\end{align*}
Notice that $p(\tau^*)=p(s_{t^*}|s_{t-1}, a_{t-1})p(s_{t-1}, a_{t-1}, ..., s_1)$
\begin{align*}
	\sum_{\tau^*} {b(s_{t^*})p(\tau^*)} 
	& = 
\end{align*}
\end{enumerate}

\begin{enumerate}
\item Solution to (a): \\
Denote $\nabla_\theta \log \pi(a_t|s_t)b(s_t)$ as $g(a_t, s_t; \theta)$; and $\pi_\theta(a_t|s_t)p(s_t|a_{t-1}, s_{t-1})$ as $q(a_t, s_t|a_{t-1}, s_{t-1})$ where $p(s_t|a_{t-1}, s_{t-1})$ is the transition dynamics. 
W.L.O.G, let's assume $a_t$, $s_t$ are discrete variable.\\
Using the chain rule, we can express $p_\theta(\tau)$ as a product of the state-action marginal $(s_t, a_t)$ and the probability of the rest of the trajectory conditioned on $(s_t, a_t)$. The derivation for the conditional expectation as follows:
\begin{align*}
	&\mathbb{E}_{p_\theta(\tau)} [\nabla_\theta \log \pi_\theta(a_t|s_t) b(s_{t})] \\
	&=\sum_{(a_1,s_1)}...\sum_{(a_t, s_t)}...\sum_{(a_T, s_T)} g(a_t, s_t; \theta)q(a_T, s_T|a_{T-1}, s_{T-1})...q(a_t, s_t|a_{t-1}, s_{t-1})...q(a_1, s_1) \\
	&= \sum_{(a_1,s_1)}...\sum_{(a_{t-1},s_{t-1})}\sum_{(a_{t+1},s_{t+1})}...\sum_{(a_T, s_T)} (\sum_{(a_t, s_t)}g(a_t, s_t; \theta)q(a_t, s_t|a_{t-1}, s_{t-1}))\\
	&\qquad q(a_T, s_T|a_{T-1}, s_{T-1})...q(a_1, s_1) \\
	&= \sum_{(a_1,s_1)}...\sum_{(a_{t-1},s_{t-1})}\sum_{(a_{t+1},s_{t+1})}...\sum_{a_T, s_T} q(a_T, s_T|a_{T-1}, s_{T-1})...q(a_1, s_1) \\
	&\qquad \sum_{(a_t, s_t)}g(a_t, s_t; \theta)q(a_t, s_t|a_{t-1}, s_{t-1}) \\
\end{align*}
And conditioned on $a_t$:
\begin{align*}
	&\sum_{(a_t, s_t)}g(a_t, s_t; \theta)q(a_t, s_t|a_{t-1}, s_{t-1}) \\
	&= \sum_{s_t} \sum_{a_t} \nabla_\theta \log \pi(a_t|s_t)b(s_t)\pi_\theta(a_t|s_t)p(s_t|a_{t-1}, s_{t-1}) \\
	&= \sum_{s_t} \sum_{a_t} b(s_t)p(s_t|a_{t-1}, s_{t-1})\nabla_\theta \log \pi(a_t|s_t)\pi_\theta(a_t|s_t) \\
	&= \sum_{s_t} b(s_t)p(s_t|a_{t-1}, s_{t-1}) \sum_{a_t}\nabla_\theta \log \pi(a_t|s_t)\pi_\theta(a_t|s_t) \\
	&= \sum_{s_t} b(s_t)p(s_t|a_{t-1}, s_{t-1}) \sum_{a_t} \nabla_\theta \pi_\theta(a_t|s_t) \\
	&= \sum_{s_t} b(s_t)p(s_t|a_{t-1}, s_{t-1}) \nabla_\theta\sum_{a_t} \pi_\theta(a_t|s_t) \\
	&= \sum_{s_t} b(s_t)p(s_t|a_{t-1}, s_{t-1}) \nabla_\theta 1 \\
	&= \sum_{s_t} b(s_t)p(s_t|a_{t-1}, s_{t-1}) 0 = 0
\end{align*}
Hence, $\mathbb{E}_{p_\theta(\tau)} [\nabla_\theta \log \pi_\theta(a_t|s_t) b(s_{t})]=0$ \\
Similarly, all the arguments can be applied on cases when $s_t$, $a_t$ are continuous variables. \\
	\\

\item Solution to (b): \\
\begin{enumerate}
\item Due to Markov Property of MDP, the future states only depend on the current state and the past is irrelevant.
\item With the same notation in (a), consider expectaion over $\tau^*=(s_1, a_1, ..., s_t, a_t)$, and then conditioned on  $(a_t, s_t)$
\begin{align*}
	&\mathbb{E}_{p_\theta(\tau^*)} [\nabla_\theta \log \pi_\theta(a_t|s_t) b(s_{t})] \\
	&=\sum_{(a_1,s_1)}...\sum_{(a_t, s_t)} g(a_t, s_t; \theta)q(a_t, s_t|a_{t-1}, s_{t-1})...q(a_1, s_1) \\
	&=\sum_{(a_1,s_1)}...\sum_{(a_{t-1}, s_{t-1})} \sum_{(a_t, s_t)}g(a_t, s_t; \theta)q(a_t, s_t|a_{t-1}, s_{t-1})q(a_{t-1}, s_t|a_{t-1}, s_{t-1})...q(a_1, s_1) \\
	&=\sum_{(a_1,s_1)}...\sum_{(a_{t-1}, s_{t-1})}q(a_{t-1}, s_t|a_{t-1}, s_{t-1})...q(a_1, s_1) \\
	&\qquad \sum_{(a_t, s_t)}g(a_t, s_t; \theta)q(a_t, s_t|a_{t-1}, s_{t-1})
\end{align*}
And again, conditioned on $a_t$ and the same argument in (a):
\begin{align*}
	&\sum_{(a_t, s_t)}g(a_t, s_t; \theta)q(a_t, s_t|a_{t-1}, s_{t-1}) \\
	&= \sum_{s_t} \sum_{a_t} \nabla_\theta \log \pi(a_t|s_t)b(s_t)\pi_\theta(a_t|s_t)p(s_t|a_{t-1}, s_{t-1}) \\
	&= \sum_{s_t} b(s_t)p(s_t|a_{t-1}, s_{t-1}) \sum_{a_t} \nabla_\theta \pi_\theta(a_t|s_t) \\
	&= \sum_{s_t} b(s_t)p(s_t|a_{t-1}, s_{t-1}) 0 = 0
\end{align*}
Similarly, all the arguments can be applied on cases when $s_t$, $a_t$ are continuous variables. \\
\end{enumerate}
\end{enumerate}


\end{document}


